{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b94e52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import set_determinism, ensure_tuple_rep\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.squeeze_and_excitation import ChannelSELayer\n",
    "from monai.utils import UpsampleMode\n",
    "from monai.handlers import CheckpointSaver\n",
    "from monai.networks.blocks.unetr_block import  UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    Compose\n",
    ")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import datetime\n",
    "from typing import Optional, Sequence, Tuple, Type, Union, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1cd8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07_15_2024 175000\n"
     ]
    }
   ],
   "source": [
    "set_determinism(seed=2023)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "x = datetime.datetime.now()\n",
    "root_dir = x.strftime(\"%m_%d_%Y %H%M%S\")\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir)\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb1b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleBlock(nn.Module):\n",
    "    def __init__(self, groups=4):\n",
    "        super(ShuffleBlock, self).__init__()\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''\n",
    "       \n",
    "        b, N, C, H, W = x.size()\n",
    "        g = self.groups\n",
    "        return x.view(b, N, g, C//g, H, W).permute(0, 2, 1, 3, 4,5).reshape(b,N, C, H, W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "489070f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch,kernel_size = 3, stride = 1,\n",
    "                 padding = 1, dilation = 1, groups = 1, bias = True):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.se_layer = ChannelSELayer(\n",
    "            spatial_dims=3, in_channels=out_ch, r=2, acti_type_1=\"relu\", acti_type_2=\"sigmoid\"\n",
    "        )\n",
    "        \n",
    "        self.relu=nn.LeakyReLU(inplace=True)\n",
    "    \n",
    "        self.cov = nn.Conv3d(in_channels = in_ch, out_channels = out_ch, kernel_size = kernel_size,\n",
    "                             stride = stride, padding = padding, dilation = dilation, groups = groups, bias = bias)\n",
    "        self.bn = nn.BatchNorm3d(num_features = out_ch)\n",
    "       \n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        return self.relu(self.se_layer(  self.bn(self.cov(input)) )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac786673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class E_CATBraTS(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: Union[Sequence[int], int],\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        depths: Sequence[int] = (2, 2, 2, 2),\n",
    "        num_heads: Sequence[int] = (3, 6, 12, 24),\n",
    "        feature_size: int = 24,\n",
    "        norm_name: Union[Tuple, str] = \"instance\",\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        dropout_path_rate: float = 0.0,\n",
    "        normalize: bool = True,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample=\"merging\",\n",
    "        init_filters: int = 8,\n",
    "        dropout_prob: Optional[float] = None,\n",
    "        act: Union[Tuple, str] = (\"RELU\", {\"inplace\": True}),\n",
    "        norm: Union[Tuple, str] = (\"GROUP\", {\"num_groups\": 8}),\n",
    "        num_groups: int = 8,\n",
    "        use_conv_final: bool = True,\n",
    "        blocks_down: tuple = (1, 2, 2, 4),\n",
    "        blocks_up: tuple = (1, 1, 1),\n",
    "        upsample_mode: Union[UpsampleMode, str] = UpsampleMode.NONTRAINABLE,\n",
    "        num_class = 2, \n",
    "        with_BN = True, \n",
    "        channel_width = 4\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = ensure_tuple_rep(img_size, spatial_dims)\n",
    "        patch_size = ensure_tuple_rep(2, spatial_dims)\n",
    "        window_size = ensure_tuple_rep(7, spatial_dims)\n",
    "\n",
    "        if not (spatial_dims == 2 or spatial_dims == 3):\n",
    "            raise ValueError(\"spatial dimension should be 2 or 3.\")\n",
    "\n",
    "        for m, p in zip(img_size, patch_size):\n",
    "            for i in range(5):\n",
    "                if m % np.power(p, i + 1) != 0:\n",
    "                    raise ValueError(\"input image size (img_size) should be divisible by stage-wise image resolution.\")\n",
    "\n",
    "        if not (0 <= drop_rate <= 1):\n",
    "            raise ValueError(\"dropout rate should be between 0 and 1.\")\n",
    "\n",
    "        if not (0 <= attn_drop_rate <= 1):\n",
    "            raise ValueError(\"attention dropout rate should be between 0 and 1.\")\n",
    "\n",
    "        if not (0 <= dropout_path_rate <= 1):\n",
    "            raise ValueError(\"drop path rate should be between 0 and 1.\")\n",
    "\n",
    "        if feature_size % 12 != 0:\n",
    "            raise ValueError(\"feature_size should be divisible by 12.\")\n",
    "\n",
    "        self.normalize = normalize\n",
    "        self.act = act\n",
    "        self.swinViT = SwinTransformer(\n",
    "            in_chans=in_channels,\n",
    "            embed_dim=feature_size,\n",
    "            window_size=window_size,\n",
    "            patch_size=patch_size,\n",
    "            depths=depths,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=4.0,\n",
    "            qkv_bias=True,\n",
    "            drop_rate=drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate,\n",
    "            drop_path_rate=dropout_path_rate,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            spatial_dims=spatial_dims\n",
    "        )\n",
    "\n",
    "    \n",
    "     \n",
    "        self.cov3d_12_en = DoubleConv(in_ch = in_channels, out_ch = 1 * feature_size, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.cov3d_22_en = DoubleConv(in_ch =  feature_size, out_ch =  feature_size, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.cov3d_32_en = DoubleConv(in_ch = 2 * feature_size, out_ch = 2 * feature_size, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.cov3d_42_en = DoubleConv(in_ch = 4 * feature_size, out_ch = 4 * feature_size, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.cov3d_4_en = DoubleConv(in_ch = 8 * feature_size, out_ch = 8 * feature_size, kernel_size = 3, padding = 1)\n",
    "  \n",
    "        self.cov3d_52_en = DoubleConv(in_ch = 16 * feature_size, out_ch = 16 * feature_size, kernel_size = 3, padding = 1)\n",
    "  \n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=16 * feature_size,\n",
    "            out_channels=8 * feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder1 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n",
    "        self.shuffle1 = ShuffleBlock()\n",
    "        self.shuffle2 = ShuffleBlock()\n",
    "        self.shuffle3 = ShuffleBlock()\n",
    "        self.shuffle4 = ShuffleBlock()\n",
    "        self.shuffle5 = ShuffleBlock()\n",
    "\n",
    "    def load_from(self, weights):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.swinViT.patch_embed.proj.weight.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.weight\"])\n",
    "            self.swinViT.patch_embed.proj.bias.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.bias\"])\n",
    "            for bname, block in self.swinViT.layers1[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers1\")\n",
    "            self.swinViT.layers1[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers1[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers1[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.bias\"]\n",
    "            )\n",
    "            for bname, block in self.swinViT.layers2[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers2\")\n",
    "            self.swinViT.layers2[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers2[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers2[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.bias\"]\n",
    "            )\n",
    "            for bname, block in self.swinViT.layers3[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers3\")\n",
    "            self.swinViT.layers3[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers3.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers3[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers3[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.bias\"]\n",
    "            )\n",
    "            for bname, block in self.swinViT.layers4[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers4\")\n",
    "            self.swinViT.layers4[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers4.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers4[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers4[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.bias\"]\n",
    "            )\n",
    "\n",
    "    def forward(self, x_in):\n",
    "     \n",
    "        \n",
    "        hidden_states_out = self.swinViT(x_in, self.normalize)\n",
    "        enc0 = self.cov3d_12_en(x_in)\n",
    "        \n",
    "        enc1 = self.shuffle1(hidden_states_out[0])\n",
    "        enc1 = self.cov3d_22_en(enc1)\n",
    "        \n",
    "        enc2 = self.shuffle2(hidden_states_out[1])\n",
    "        enc2 = self.cov3d_32_en(enc2)\n",
    "        \n",
    "        enc3 = self.shuffle3(hidden_states_out[2])\n",
    "        enc3 = self.cov3d_42_en(enc3)\n",
    "        \n",
    "        enc4 = self.shuffle4(hidden_states_out[3])\n",
    "        enc4 = self.cov3d_4_en(enc4)\n",
    "        \n",
    "        dec4 = self.shuffle5(hidden_states_out[4])\n",
    "        dec4 = self.cov3d_52_en(dec4)\n",
    "        \n",
    "        \n",
    "        dec3 = self.decoder5(dec4,enc4)\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        dec0 = self.decoder2(dec1, enc1)\n",
    "        out = self.decoder1(dec0, enc0)\n",
    "        logits = self.out(out)\n",
    "     \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03ab4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SWIN TRANSFORMER AVERAGEMETER\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = np.where(self.count > 0, self.sum / self.count, self.sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "497daa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LoadDataset(img_path,roi):\n",
    "    \"\"\"TOTAL 500 cases, 350 training, 100 val, 50 eval\"\"\" \n",
    "    patients = os.listdir(img_path)\n",
    "    random.shuffle(patients)\n",
    "    images = []\n",
    " \n",
    "    for patient in patients:\n",
    "        \n",
    "        p = patient.removesuffix(\"nifti\")\n",
    "        \n",
    "        T1 = img_path+\"/\"+patient+\"/\"+p+\"T1.nii.gz\"\n",
    "        T1GD = img_path+\"/\"+patient+\"/\"+p+\"T1c.nii.gz\"\n",
    "        T2 = img_path+\"/\"+patient+\"/\"+p+\"T2.nii.gz\"\n",
    "        TFlair = img_path+\"/\"+patient+\"/\"+p+\"FLAIR.nii.gz\"\n",
    "     \n",
    "        mask = img_path+\"/\"+patient+\"/\"+p+\"tumor_segmentation.nii.gz\"\n",
    "        \n",
    "        images.append({\"image\":[T1,T1GD,T2,TFlair],\"label\":[mask]})\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    train_dt = images[0:350]\n",
    "    val_dt = images[350:451]\n",
    "    test_dt = images[451:500]\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            transforms.CropForegroundd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                source_key=\"image\",\n",
    "                k_divisible=[roi[0], roi[1], roi[2]],\n",
    "            ),\n",
    "              transforms.RandSpatialCropd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                roi_size=[roi[0], roi[1], roi[2]],\n",
    "                random_size=False,\n",
    "            ),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "            transforms.NormalizeIntensityd(\n",
    "                keys=\"image\", nonzero=True, channel_wise=True\n",
    "            ),\n",
    "            transforms.RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "            transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "\n",
    "        ]\n",
    "    )\n",
    "    val_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            transforms.NormalizeIntensityd(\n",
    "                keys=\"image\", nonzero=True, channel_wise=True\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_ds = data.Dataset(data=train_dt, transform=train_transform)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_ds = data.Dataset(data=val_dt, transform=val_transform)\n",
    "    val_loader = data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "    test_ds = data.Dataset(data=test_dt, transform=test_transform)\n",
    "\n",
    "    test_loader = data.DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader,test_loader,test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0254b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = E_CATBraTS(\n",
    "    img_size=roi,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    feature_size=48,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    use_checkpoint=True,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "loss_function = DiceLoss(sigmoid=True)\n",
    "\n",
    "\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "window_inferer = SlidingWindowInferer(roi_size = roi, overlap=0.5)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, epoch, loss_func):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    run_loss = AverageMeter()\n",
    "    for idx, batch_data in enumerate(loader):\n",
    "        data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        logits = model(data)\n",
    "        loss = loss_func(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        run_loss.update(loss.item(), n=batch_size)\n",
    "        print(\n",
    "            \"Epoch {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
    "            \"loss: {:.4f}\".format(run_loss.avg),\n",
    "            \"time {:.2f}s\".format(time.time() - start_time),\n",
    "        )\n",
    "        start_time = time.time()\n",
    "    return run_loss.avg\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    epoch,\n",
    "    acc_func,\n",
    "    model_inferer=None,\n",
    "    post_sigmoid=None,\n",
    "    post_pred=None,\n",
    "):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    run_acc = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch_data in enumerate(loader):\n",
    "            data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(\n",
    "                device\n",
    "            )\n",
    "            logits = model_inferer(data)\n",
    "            val_labels_list = decollate_batch(target)\n",
    "            val_outputs_list = decollate_batch(logits)\n",
    "            val_output_convert = [\n",
    "                post_pred(post_sigmoid(val_pred_tensor))\n",
    "                for val_pred_tensor in val_outputs_list\n",
    "            ]\n",
    "            acc_func.reset()\n",
    "            acc_func(y_pred=val_output_convert, y=val_labels_list)\n",
    "            acc, not_nans = acc_func.aggregate()\n",
    "            run_acc.update(acc.cpu().numpy(), n=not_nans.cpu().numpy())\n",
    "            Dice_TC = run_acc.avg[0]\n",
    "            Dice_WT = run_acc.avg[1]\n",
    "            Dice_ET = run_acc.avg[2]\n",
    "            print(\n",
    "                \"Val {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
    "                \", Dice_TC:\",\n",
    "                Dice_TC,\n",
    "                \", Dice_WT:\",\n",
    "                Dice_WT,\n",
    "                \", Dice_ET:\",\n",
    "                Dice_ET,\n",
    "                \", time {:.2f}s\".format(time.time() - start_time),\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "    return run_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "val_acc_max = 0.0\n",
    "Dices_TC = []\n",
    "Dices_WT = []\n",
    "Dices_ET = []\n",
    "Dices_avg = []\n",
    "loss_epochs = []\n",
    "trains_epoch = []\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    print(time.ctime(), \"Epoch:\", epoch)\n",
    "    epoch_time = time.time()\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        epoch=epoch,\n",
    "        loss_func=loss_func,\n",
    "    )\n",
    "    print(\n",
    "        \"Final training  {}/{}\".format(epoch, max_epochs - 1),\n",
    "        \"loss: {:.4f}\".format(train_loss),\n",
    "        \"time {:.2f}s\".format(time.time() - epoch_time),\n",
    "    )\n",
    "\n",
    "    if (epoch + 1) % val_every == 0 or epoch == 0:\n",
    "        loss_epochs.append(train_loss)\n",
    "        trains_epoch.append(int(epoch))\n",
    "        epoch_time = time.time()\n",
    "        val_acc = val_epoch(\n",
    "            model,\n",
    "            val_loader,\n",
    "            epoch=epoch,\n",
    "            acc_func=acc_func,\n",
    "            model_inferer=model_inferer,\n",
    "            post_sigmoid=post_sigmoid,\n",
    "            post_pred=post_pred,\n",
    "        )\n",
    "        Dice_TC = val_acc[0]\n",
    "        Dice_WT = val_acc[1]\n",
    "        Dice_ET = val_acc[2]\n",
    "        val_avg_acc = np.mean(val_acc)\n",
    "        print(\n",
    "            \"Final validation stats {}/{}\".format(epoch, max_epochs - 1),\n",
    "            \", Dice_TC:\",\n",
    "            Dice_TC,\n",
    "            \", Dice_WT:\",\n",
    "            Dice_WT,\n",
    "            \", Dice_ET:\",\n",
    "            Dice_ET,\n",
    "            \", Dice_Avg:\",\n",
    "            val_avg_acc,\n",
    "            \", time {:.2f}s\".format(time.time() - epoch_time),\n",
    "        )\n",
    "        Dices_TC.append(Dice_TC)\n",
    "        Dices_WT.append(Dice_WT)\n",
    "        Dices_ET.append(Dice_ET)\n",
    "        Dices_avg.append(val_avg_acc)\n",
    "        if val_avg_acc > val_acc_max:\n",
    "            print(\"new best ({:.6f} --> {:.6f}). \".format(val_acc_max, val_avg_acc))\n",
    "            val_acc_max = val_avg_acc\n",
    "            save_checkpoint(\n",
    "                model,\n",
    "                epoch,\n",
    "                best_acc=val_acc_max,\n",
    "            )\n",
    "        scheduler.step()\n",
    "print(\"Training Finished !, Best Accuracy: \", val_acc_max)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
